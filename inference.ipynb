{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6675a9ce",
   "metadata": {
    "papermill": {
     "duration": 0.003312,
     "end_time": "2023-10-15T16:08:05.286505",
     "exception": false,
     "start_time": "2023-10-15T16:08:05.283193",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Trading at the Close - Inference\n",
    "-----------------------\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c322aff9",
   "metadata": {
    "papermill": {
     "duration": 0.002489,
     "end_time": "2023-10-15T16:08:05.293598",
     "exception": false,
     "start_time": "2023-10-15T16:08:05.291109",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a690d768",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-10-15T16:08:05.300923Z",
     "iopub.status.busy": "2023-10-15T16:08:05.300510Z",
     "iopub.status.idle": "2023-10-15T16:08:05.685263Z",
     "shell.execute_reply": "2023-10-15T16:08:05.684125Z"
    },
    "papermill": {
     "duration": 0.391445,
     "end_time": "2023-10-15T16:08:05.687732",
     "exception": false,
     "start_time": "2023-10-15T16:08:05.296287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef98ce2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T16:08:05.695112Z",
     "iopub.status.busy": "2023-10-15T16:08:05.694610Z",
     "iopub.status.idle": "2023-10-15T16:08:05.701183Z",
     "shell.execute_reply": "2023-10-15T16:08:05.699870Z"
    },
    "papermill": {
     "duration": 0.013523,
     "end_time": "2023-10-15T16:08:05.704159",
     "exception": false,
     "start_time": "2023-10-15T16:08:05.690636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\\\kaggle\\\\input\\\\optiver-inference-utils', 'c:\\\\Users\\\\salva\\\\OneDrive\\\\Documentos\\\\Python Projects\\\\trading-at-the-close', 'C:\\\\Program Files\\\\Python\\\\python310.zip', 'C:\\\\Program Files\\\\Python\\\\DLLs', 'C:\\\\Program Files\\\\Python\\\\lib', 'C:\\\\Program Files\\\\Python', 'c:\\\\Users\\\\salva\\\\OneDrive\\\\Documentos\\\\Python Projects\\\\trading-at-the-close\\\\.venv', '', 'c:\\\\Users\\\\salva\\\\OneDrive\\\\Documentos\\\\Python Projects\\\\trading-at-the-close\\\\.venv\\\\lib\\\\site-packages', 'c:\\\\Users\\\\salva\\\\OneDrive\\\\Documentos\\\\Python Projects\\\\trading-at-the-close\\\\.venv\\\\lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\salva\\\\OneDrive\\\\Documentos\\\\Python Projects\\\\trading-at-the-close\\\\.venv\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\salva\\\\OneDrive\\\\Documentos\\\\Python Projects\\\\trading-at-the-close\\\\.venv\\\\lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "utils_path = Path(\"/\", \"kaggle\", \"input\", \"optiver-inference-utils\")\n",
    "if str(utils_path) not in sys.path:\n",
    "    sys.path = [str(utils_path),] + sys.path\n",
    "    \n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbcfe055",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T16:08:05.711107Z",
     "iopub.status.busy": "2023-10-15T16:08:05.710781Z",
     "iopub.status.idle": "2023-10-15T16:08:05.716012Z",
     "shell.execute_reply": "2023-10-15T16:08:05.714923Z"
    },
    "papermill": {
     "duration": 0.011112,
     "end_time": "2023-10-15T16:08:05.718081",
     "exception": false,
     "start_time": "2023-10-15T16:08:05.706969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    LOCAL = True\n",
    "    JOBS_PATH_LOCAL = Path(\".\", \"job_artifacts\")\n",
    "    JOBS_PATH_ONLINE = Path(\"/\", \"kaggle\", \"input\", \"optiver-trained-artifacts\", \"job_artifacts\")\n",
    "    TEST_PATH = Path(\".\", \"train_files\", \"train.csv\")\n",
    "    INPUT_METADATA_CASE = \"optiver-feature_selection-0010\"\n",
    "    INPUT_METADATA_NAME = \"output_metadata.json\"\n",
    "    MODEL_CASE = \"optiver-feature_selection-0010\"\n",
    "    CACHE_HORIZON = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bf842d",
   "metadata": {
    "papermill": {
     "duration": 0.002372,
     "end_time": "2023-10-15T16:08:05.723299",
     "exception": false,
     "start_time": "2023-10-15T16:08:05.720927",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd4774a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T16:08:05.730096Z",
     "iopub.status.busy": "2023-10-15T16:08:05.729713Z",
     "iopub.status.idle": "2023-10-15T16:08:05.734994Z",
     "shell.execute_reply": "2023-10-15T16:08:05.733846Z"
    },
    "papermill": {
     "duration": 0.011263,
     "end_time": "2023-10-15T16:08:05.737129",
     "exception": false,
     "start_time": "2023-10-15T16:08:05.725866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices)/np.sum(std_error)\n",
    "    out = prices - std_error * step\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_mean(prices):\n",
    "    prices -= prices.mean()\n",
    "    return prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fc48e42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T16:08:05.780569Z",
     "iopub.status.busy": "2023-10-15T16:08:05.780186Z",
     "iopub.status.idle": "2023-10-15T16:08:05.785998Z",
     "shell.execute_reply": "2023-10-15T16:08:05.784973Z"
    },
    "papermill": {
     "duration": 0.011937,
     "end_time": "2023-10-15T16:08:05.788117",
     "exception": false,
     "start_time": "2023-10-15T16:08:05.776180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(\n",
    "    model_type,\n",
    "    booster_file\n",
    "):\n",
    "    model = model_type(model_file=str(booster_file.with_suffix(\".txt\")))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating mock API...\n",
      "Mock API created.\n",
      "Test date range: 478 - 480 (3 days)\n"
     ]
    }
   ],
   "source": [
    "# Online API\n",
    "if not CFG.LOCAL:\n",
    "\n",
    "    import optiver2023\n",
    "    optiver2023.make_env.func_dict['__called__'] = False # This enables running the API again if an error was produced\n",
    "\n",
    "    env = optiver2023.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "\n",
    "    CFG.JOBS_PATH = CFG.JOBS_PATH_ONLINE\n",
    "\n",
    "# Local API\n",
    "else:\n",
    "    from utils.public_timeseries_testing_util import MockApi\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.read_csv(CFG.TEST_PATH)\n",
    "    env = MockApi(df, start_date=478, end_date=480)\n",
    "    iter_test = env.iter_test()\n",
    "\n",
    "    CFG.JOBS_PATH = CFG.JOBS_PATH_LOCAL\n",
    "\n",
    "CFG.INPUT_METADATA_PATH = CFG.JOBS_PATH.joinpath(CFG.INPUT_METADATA_CASE, CFG.INPUT_METADATA_NAME)\n",
    "CFG.MODEL_PATH = CFG.JOBS_PATH.joinpath(CFG.MODEL_CASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from loguru import logger\n",
    "    import sys\n",
    "    logger.remove()\n",
    "    logger.add(sys.stdout, level=\"ERROR\")\n",
    "except:\n",
    "    import logging\n",
    "    logger = logging.getLogger(\"__main__\")\n",
    "    logger.setLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "225ac5bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-15T16:08:05.795554Z",
     "iopub.status.busy": "2023-10-15T16:08:05.795184Z",
     "iopub.status.idle": "2023-10-15T16:08:53.052733Z",
     "shell.execute_reply": "2023-10-15T16:08:53.051619Z"
    },
    "papermill": {
     "duration": 47.264671,
     "end_time": "2023-10-15T16:08:53.055745",
     "exception": false,
     "start_time": "2023-10-15T16:08:05.791074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date ID 478, second in bucket 0 (took 0.91s)\n",
      "Date ID 478, second in bucket 10 (took 0.93s)\n",
      "Date ID 478, second in bucket 20 (took 0.98s)\n",
      "Date ID 478, second in bucket 30 (took 1.07s)\n",
      "Date ID 478, second in bucket 40 (took 1.14s)\n",
      "Date ID 478, second in bucket 50 (took 1.25s)\n",
      "Date ID 478, second in bucket 60 (took 1.46s)\n",
      "Date ID 478, second in bucket 70 (took 1.40s)\n",
      "Date ID 478, second in bucket 80 (took 1.53s)\n",
      "Date ID 478, second in bucket 90 (took 1.57s)\n",
      "Date ID 478, second in bucket 100 (took 1.61s)\n",
      "Date ID 478, second in bucket 110 (took 1.63s)\n",
      "Date ID 478, second in bucket 120 (took 1.60s)\n",
      "Date ID 478, second in bucket 130 (took 1.59s)\n",
      "Date ID 478, second in bucket 140 (took 1.62s)\n",
      "Date ID 478, second in bucket 150 (took 1.62s)\n",
      "Date ID 478, second in bucket 160 (took 1.62s)\n",
      "Date ID 478, second in bucket 170 (took 1.60s)\n",
      "Date ID 478, second in bucket 180 (took 1.61s)\n",
      "Date ID 478, second in bucket 190 (took 1.60s)\n",
      "Date ID 478, second in bucket 200 (took 1.60s)\n",
      "Date ID 478, second in bucket 210 (took 1.74s)\n",
      "Date ID 478, second in bucket 220 (took 1.62s)\n",
      "Date ID 478, second in bucket 230 (took 1.61s)\n",
      "Date ID 478, second in bucket 240 (took 1.66s)\n",
      "Date ID 478, second in bucket 250 (took 1.60s)\n",
      "Date ID 478, second in bucket 260 (took 1.62s)\n",
      "Date ID 478, second in bucket 270 (took 1.68s)\n",
      "Date ID 478, second in bucket 280 (took 1.63s)\n",
      "Date ID 478, second in bucket 290 (took 1.60s)\n",
      "Date ID 478, second in bucket 300 (took 1.61s)\n",
      "Date ID 478, second in bucket 310 (took 1.62s)\n",
      "Date ID 478, second in bucket 320 (took 1.63s)\n",
      "Date ID 478, second in bucket 330 (took 1.62s)\n",
      "Date ID 478, second in bucket 340 (took 1.62s)\n",
      "Date ID 478, second in bucket 350 (took 1.74s)\n",
      "Date ID 478, second in bucket 360 (took 1.60s)\n",
      "Date ID 478, second in bucket 370 (took 1.62s)\n",
      "Date ID 478, second in bucket 380 (took 1.61s)\n",
      "Date ID 478, second in bucket 390 (took 1.62s)\n",
      "Date ID 478, second in bucket 400 (took 1.65s)\n",
      "Date ID 478, second in bucket 410 (took 1.62s)\n",
      "Date ID 478, second in bucket 420 (took 1.63s)\n",
      "Date ID 478, second in bucket 430 (took 1.64s)\n",
      "Date ID 478, second in bucket 440 (took 1.61s)\n",
      "Date ID 478, second in bucket 450 (took 1.64s)\n",
      "Date ID 478, second in bucket 460 (took 1.70s)\n",
      "Date ID 478, second in bucket 470 (took 1.63s)\n",
      "Date ID 478, second in bucket 480 (took 1.63s)\n",
      "Date ID 478, second in bucket 490 (took 1.79s)\n",
      "Date ID 478, second in bucket 500 (took 1.63s)\n",
      "Date ID 478, second in bucket 510 (took 1.63s)\n",
      "Date ID 478, second in bucket 520 (took 1.65s)\n",
      "Date ID 478, second in bucket 530 (took 1.62s)\n",
      "Date ID 478, second in bucket 540 (took 1.62s)\n",
      "Date ID 479, second in bucket 0 (took 1.73s)\n",
      "Date ID 479, second in bucket 10 (took 1.70s)\n",
      "Date ID 479, second in bucket 20 (took 1.69s)\n",
      "Date ID 479, second in bucket 30 (took 1.71s)\n",
      "Date ID 479, second in bucket 40 (took 1.72s)\n",
      "Date ID 479, second in bucket 50 (took 1.72s)\n",
      "Date ID 479, second in bucket 60 (took 1.70s)\n",
      "Date ID 479, second in bucket 70 (took 1.74s)\n",
      "Date ID 479, second in bucket 80 (took 1.84s)\n",
      "Date ID 479, second in bucket 90 (took 1.70s)\n",
      "Date ID 479, second in bucket 100 (took 1.62s)\n",
      "Date ID 479, second in bucket 110 (took 1.61s)\n",
      "Date ID 479, second in bucket 120 (took 1.61s)\n",
      "Date ID 479, second in bucket 130 (took 1.59s)\n",
      "Date ID 479, second in bucket 140 (took 1.60s)\n",
      "Date ID 479, second in bucket 150 (took 1.60s)\n",
      "Date ID 479, second in bucket 160 (took 1.63s)\n",
      "Date ID 479, second in bucket 170 (took 1.60s)\n",
      "Date ID 479, second in bucket 180 (took 1.61s)\n",
      "Date ID 479, second in bucket 190 (took 1.66s)\n",
      "Date ID 479, second in bucket 200 (took 1.60s)\n",
      "Date ID 479, second in bucket 210 (took 1.58s)\n",
      "Date ID 479, second in bucket 220 (took 1.62s)\n",
      "Date ID 479, second in bucket 230 (took 1.72s)\n",
      "Date ID 479, second in bucket 240 (took 1.60s)\n",
      "Date ID 479, second in bucket 250 (took 1.67s)\n",
      "Date ID 479, second in bucket 260 (took 1.60s)\n",
      "Date ID 479, second in bucket 270 (took 1.62s)\n",
      "Date ID 479, second in bucket 280 (took 1.60s)\n",
      "Date ID 479, second in bucket 290 (took 1.61s)\n",
      "Date ID 479, second in bucket 300 (took 1.61s)\n",
      "Date ID 479, second in bucket 310 (took 1.62s)\n",
      "Date ID 479, second in bucket 320 (took 1.62s)\n",
      "Date ID 479, second in bucket 330 (took 1.65s)\n",
      "Date ID 479, second in bucket 340 (took 1.64s)\n",
      "Date ID 479, second in bucket 350 (took 1.63s)\n",
      "Date ID 479, second in bucket 360 (took 1.61s)\n",
      "Date ID 479, second in bucket 370 (took 1.76s)\n",
      "Date ID 479, second in bucket 380 (took 1.62s)\n",
      "Date ID 479, second in bucket 390 (took 1.60s)\n",
      "Date ID 479, second in bucket 400 (took 1.62s)\n",
      "Date ID 479, second in bucket 410 (took 1.65s)\n",
      "Date ID 479, second in bucket 420 (took 1.63s)\n",
      "Date ID 479, second in bucket 430 (took 1.63s)\n",
      "Date ID 479, second in bucket 440 (took 1.65s)\n",
      "Date ID 479, second in bucket 450 (took 1.63s)\n",
      "Date ID 479, second in bucket 460 (took 1.63s)\n",
      "Date ID 479, second in bucket 470 (took 1.66s)\n",
      "Date ID 479, second in bucket 480 (took 1.65s)\n",
      "Date ID 479, second in bucket 490 (took 1.64s)\n",
      "Date ID 479, second in bucket 500 (took 1.61s)\n",
      "Date ID 479, second in bucket 510 (took 1.76s)\n",
      "Date ID 479, second in bucket 520 (took 1.62s)\n",
      "Date ID 479, second in bucket 530 (took 1.64s)\n",
      "Date ID 479, second in bucket 540 (took 1.63s)\n",
      "Date ID 480, second in bucket 0 (took 1.73s)\n",
      "Date ID 480, second in bucket 10 (took 1.70s)\n",
      "Date ID 480, second in bucket 20 (took 1.71s)\n",
      "Date ID 480, second in bucket 30 (took 1.72s)\n",
      "Date ID 480, second in bucket 40 (took 1.70s)\n",
      "Date ID 480, second in bucket 50 (took 1.72s)\n",
      "Date ID 480, second in bucket 60 (took 1.70s)\n",
      "Date ID 480, second in bucket 70 (took 1.74s)\n",
      "Date ID 480, second in bucket 80 (took 1.73s)\n",
      "Date ID 480, second in bucket 90 (took 1.81s)\n",
      "Date ID 480, second in bucket 100 (took 1.60s)\n",
      "Date ID 480, second in bucket 110 (took 1.76s)\n",
      "Date ID 480, second in bucket 120 (took 1.61s)\n",
      "Date ID 480, second in bucket 130 (took 1.62s)\n",
      "Date ID 480, second in bucket 140 (took 1.63s)\n",
      "Date ID 480, second in bucket 150 (took 1.65s)\n",
      "Date ID 480, second in bucket 160 (took 1.62s)\n",
      "Date ID 480, second in bucket 170 (took 1.61s)\n",
      "Date ID 480, second in bucket 180 (took 1.62s)\n",
      "Date ID 480, second in bucket 190 (took 1.65s)\n",
      "Date ID 480, second in bucket 200 (took 1.64s)\n",
      "Date ID 480, second in bucket 210 (took 1.59s)\n",
      "Date ID 480, second in bucket 220 (took 1.57s)\n",
      "Date ID 480, second in bucket 230 (took 1.65s)\n",
      "Date ID 480, second in bucket 240 (took 1.62s)\n",
      "Date ID 480, second in bucket 250 (took 1.77s)\n",
      "Date ID 480, second in bucket 260 (took 1.64s)\n",
      "Date ID 480, second in bucket 270 (took 1.61s)\n",
      "Date ID 480, second in bucket 280 (took 1.61s)\n",
      "Date ID 480, second in bucket 290 (took 1.67s)\n",
      "Date ID 480, second in bucket 300 (took 1.61s)\n",
      "Date ID 480, second in bucket 310 (took 1.60s)\n",
      "Date ID 480, second in bucket 320 (took 1.62s)\n",
      "Date ID 480, second in bucket 330 (took 1.64s)\n",
      "Date ID 480, second in bucket 340 (took 1.59s)\n",
      "Date ID 480, second in bucket 350 (took 1.60s)\n",
      "Date ID 480, second in bucket 360 (took 1.60s)\n",
      "Date ID 480, second in bucket 370 (took 1.61s)\n",
      "Date ID 480, second in bucket 380 (took 1.61s)\n",
      "Date ID 480, second in bucket 390 (took 1.64s)\n",
      "Date ID 480, second in bucket 400 (took 1.76s)\n",
      "Date ID 480, second in bucket 410 (took 1.65s)\n",
      "Date ID 480, second in bucket 420 (took 1.63s)\n",
      "Date ID 480, second in bucket 430 (took 1.70s)\n",
      "Date ID 480, second in bucket 440 (took 1.68s)\n",
      "Date ID 480, second in bucket 450 (took 1.69s)\n",
      "Date ID 480, second in bucket 460 (took 1.70s)\n",
      "Date ID 480, second in bucket 470 (took 1.73s)\n",
      "Date ID 480, second in bucket 480 (took 1.74s)\n",
      "Date ID 480, second in bucket 490 (took 1.62s)\n",
      "Date ID 480, second in bucket 500 (took 1.60s)\n",
      "Date ID 480, second in bucket 510 (took 1.65s)\n",
      "Date ID 480, second in bucket 520 (took 1.56s)\n",
      "Date ID 480, second in bucket 530 (took 1.58s)\n",
      "Date ID 480, second in bucket 540 (took 1.71s)\n"
     ]
    }
   ],
   "source": [
    "from utils.files import read_json\n",
    "from utils.features import feature_engineering, select_features\n",
    "from lightgbm import Booster\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "metadata = read_json(CFG.INPUT_METADATA_PATH)\n",
    "stock_weights = {int(k): v for k, v in metadata[\"stock_weights\"].items()}\n",
    "stock_clusters = {int(k): v for k, v in metadata[\"stock_clusters\"].items()}\n",
    "selected_features = metadata[\"selected_features\"]\n",
    "\n",
    "counter = 0\n",
    "predictions = []\n",
    "\n",
    "models_boosters = CFG.MODEL_PATH.glob(\"**/*.txt\")\n",
    "models = [load_model(Booster, path) for path in models_boosters]\n",
    "\n",
    "cache_test = pd.DataFrame()\n",
    "cache_revealed_targets = None\n",
    "for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "\n",
    "    start = timer()\n",
    "\n",
    "    current_date_id = test[\"date_id\"].iloc[0]\n",
    "    current_second_in_bucket = test[\"seconds_in_bucket\"].iloc[0]\n",
    "\n",
    "    # Save revealed target when available\n",
    "    if revealed_targets.shape[0] > 1:\n",
    "        cache_revealed_targets = revealed_targets\n",
    "\n",
    "    # Cache inference data (without feature engineering for memory reasons)\n",
    "    cache_test = pd.concat([cache_test, test], ignore_index=True, axis=0)\n",
    "    if counter > 0:\n",
    "        cache_test = cache_test.groupby([\"stock_id\"]).tail(CFG.CACHE_HORIZON).sort_values(\n",
    "                by=[\"date_id\", \"seconds_in_bucket\", \"stock_id\"]).reset_index(drop=True)\n",
    "\n",
    "    # Do feature engineering over cache and take only the data relevant for inference in this timestep (revealed target is introduced by other means)\n",
    "    feat = feature_engineering(cache_test, revealed_target=False, weights=stock_weights, clusters=stock_clusters)[-test.shape[0]:]\n",
    "    if cache_revealed_targets is not None:\n",
    "        feat[\"revealed_target\"] = cache_revealed_targets[\"revealed_target\"].loc[\n",
    "            (cache_revealed_targets[\"revealed_date_id\"] == current_date_id - 1) & (cache_revealed_targets[\"seconds_in_bucket\"] == current_second_in_bucket)]\n",
    "    else:\n",
    "        feat[\"revealed_target\"] = np.nan\n",
    "\n",
    "    feat = select_features(feat, selected_features, reduce_memory=False)\n",
    "\n",
    "    # Perform prediction as a mean ensemble\n",
    "    prediction = 0\n",
    "    for model in models:\n",
    "        prediction += model.predict(feat)   \n",
    "    prediction /= len(models)\n",
    "\n",
    "    # Do postprocessing over the redictions\n",
    "    prediction = zero_sum(prediction, test[\"bid_size\"] + test[\"ask_size\"])\n",
    "    # prediction = zero_mean(prediction)\n",
    "\n",
    "    end = timer()\n",
    "    print(f\"Date ID {current_date_id}, second in bucket {current_second_in_bucket} (took {end - start:.2f}s)\")\n",
    "\n",
    "    # Save prediction\n",
    "    sample_prediction[\"target\"] = prediction\n",
    "    env.predict(sample_prediction)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 51.832865,
   "end_time": "2023-10-15T16:08:53.787762",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-15T16:08:01.954897",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
